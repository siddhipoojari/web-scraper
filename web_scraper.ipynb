{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "42ba8c3d-f407-4356-bf3f-4cb6960579c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bf8689-b282-478f-b58e-0b82e4816345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# URL of the login page\n",
    "login_url = \"your url\"\n",
    "driver.get(login_url)\n",
    "\n",
    "# Wait for the login page to load\n",
    "WebDriverWait(driver, 20).until(\n",
    "    EC.presence_of_element_located((By.ID, 'user-email'))\n",
    ")\n",
    "\n",
    "# Input username and password\n",
    "username = \"your email\"\n",
    "password = \"your password\"\n",
    "\n",
    "username_field = driver.find_element(By.ID, 'user-email')\n",
    "password_field = driver.find_element(By.ID, 'user-password')\n",
    "\n",
    "username_field.send_keys(username)\n",
    "password_field.send_keys(password)\n",
    "\n",
    "# Submit the login form\n",
    "login_button = driver.find_element(By.XPATH, '//*[@id=\"main-container\"]/div[2]/div/div/div/div/div/div/form/div/div/div[3]/div/div/button') #id of my login button\n",
    "login_button.click()\n",
    "\n",
    "# Wait for the main page to load after login\n",
    "main_page_url = \"your url\"\n",
    "WebDriverWait(driver, 10).until(\n",
    "    EC.url_to_be(main_page_url)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1e4312a7-f207-4d6d-beff-bbfcc722254c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hrefs = []\n",
    "\n",
    "def extract_hrefs():\n",
    "    try:\n",
    "        tbody = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, '//*[@id=\"app\"]/div[1]/main/div[2]/div/div/div/div/div[2]/table/tbody')) #fetching table body\n",
    "            \n",
    "        # //*[@id=\"main-container\"]/div/div[2]/div/div/div[2]/table/tbody\n",
    "        )\n",
    "        rows = tbody.find_elements(By.TAG_NAME, 'tr')\n",
    "        for row in rows:\n",
    "            status_element = row.find_element(By.XPATH, './/span[@data-testid=\"display-status\"]')\n",
    "            status = status_element.text.strip()\n",
    "            if status == \"Sent\": # only extracting emails which have been sent\n",
    "                a_elements = row.find_elements(By.TAG_NAME, 'a')\n",
    "                for a in a_elements:\n",
    "                    href = a.get_attribute('href') #extracting and storing email specific links\n",
    "                    if href and href not in hrefs:\n",
    "                        hrefs.append(href)\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting hrefs: {e}\")\n",
    "\n",
    "def get_page_content():\n",
    "    try:\n",
    "        tbody = driver.find_element(By.XPATH, '//*[@id=\"app\"]/div[1]/main/div[2]/div/div/div/div/div[2]/table/tbody')\n",
    "        return tbody.text\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting page content: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Extract hrefs from the first page\n",
    "extract_hrefs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e1ef70-5d11-46c2-b472-0a7b67b3fcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop to click 'Next' button and extract hrefs from subsequent pages\n",
    "page_num = 1\n",
    "previous_page_content = get_page_content()\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        # Wait for the 'Next' button to be clickable\n",
    "        next_button = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, '//button[contains(@class, \"btn\") and .//i[contains(@class, \"fa-chevron-right\")]]'))\n",
    "        ) # extracting next button through x path\n",
    "\n",
    "        # Click the 'Next' button\n",
    "        next_button.click()\n",
    "        page_num += 1\n",
    "        print(f\"Extracting hrefs from page {page_num}\")\n",
    "        \n",
    "        # Wait for the page to load\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, '//*[@id=\"app\"]/div[1]/main/div[2]/div/div/div/div/div[2]/table/tbody'))\n",
    "        )\n",
    "\n",
    "        current_page_content = get_page_content()\n",
    "        \n",
    "        if current_page_content == previous_page_content:\n",
    "            print(\"Reached the last page. Exiting loop.\")\n",
    "            break\n",
    "\n",
    "        previous_page_content = current_page_content\n",
    "\n",
    "        extract_hrefs()\n",
    "    except Exception as e:\n",
    "        print(f\"No more pages or encountered an error: {e}\")\n",
    "        break\n",
    "\n",
    "# Print all extracted hrefs\n",
    "for href in hrefs:\n",
    "    print(href)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4b7bb4-8cff-4b7f-aaa1-d229872a313e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape data from each page\n",
    "def scrape_data(url):\n",
    "    driver.get(url)\n",
    "    try:\n",
    "        # Wait for the page to load\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, 'row.items-push'))\n",
    "        )\n",
    "\n",
    "        data = {}\n",
    "        parent_element = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, '//label[text()=\"Email Marketing\"]/..'))\n",
    "        )\n",
    "        title_element = parent_element.find_element(By.XPATH, './/span[@data-testid=\"display-name\"]')\n",
    "        data['Title'] = title_element.text.strip()\n",
    "        data['Subject'] = \"Subject not found or empty\"  # Default value\n",
    "        recipients_text = driver.find_element(By.CLASS_NAME, 'col-12.font-size-h5.font-w500').text.strip()\n",
    "        recipients_num = re.search(r'\\((\\d{1,3}(?:,\\d{3})*) Recipients\\)', recipients_text)\n",
    "        if recipients_num:\n",
    "            data['Total Recipients'] = recipients_num.group(1).replace(',', '')  \n",
    "        else:\n",
    "            data['Total Recipients'] = \"0\"\n",
    "\n",
    "        # Extract Successful Deliveries\n",
    "        deliveries_text = driver.find_element(By.XPATH, '//a[contains(text(),\"Delivered\")]/following-sibling::div').text.strip()\n",
    "        deliveries_match = re.match(r'\\d+% \\(([\\d,]+)\\)', deliveries_text)\n",
    "        if deliveries_match:\n",
    "            data['Successful Deliveries'] = deliveries_match.group(1).replace(',', '')\n",
    "            \n",
    "        opened_text = driver.find_element(By.XPATH, '//a[contains(text(),\"Opened\")]/following-sibling::div').text.strip()\n",
    "        opened_match = re.match(r'(\\d+)% \\(([\\d,]+)\\)', opened_text)\n",
    "        if opened_match:\n",
    "            data['Open Rate'] = opened_match.group(1) + '%'\n",
    "            data['Total Opens'] = opened_match.group(2).replace(',', '')\n",
    "            \n",
    "\n",
    "        clicked_text = driver.find_element(By.XPATH, '//a[contains(text(),\"Clicked\")]/following-sibling::div').text.strip()\n",
    "        clicked_match = re.match(r'(\\d+)% \\(([\\d,]+)\\)', clicked_text)\n",
    "        if clicked_match:\n",
    "            data['Click Rate'] = clicked_match.group(1) + '%'\n",
    "            data['Total Clicks'] = clicked_match.group(2).replace(',', '')\n",
    "            \n",
    "        data['Unsubscribes'] = driver.find_element(By.XPATH, '//a[contains(text(),\"Unsubscribed\")]/preceding-sibling::div').text.strip()\n",
    "        data['Total Bounces'] = driver.find_element(By.XPATH, '//a[contains(text(),\"Bounced\")]/preceding-sibling::div').text.strip()\n",
    "        data['Unopened'] = driver.find_element(By.XPATH, '//a[contains(text(),\"Unopened\")]/preceding-sibling::div').text.strip()\n",
    "        sent_datetime = driver.find_element(By.XPATH, '//div[@class=\"mt-2\"]/span[@class=\"text-success\"]').text.strip()\n",
    "        date, time = sent_datetime.split(' - ')\n",
    "        datetime_obj = datetime.strptime(f\"{date} {time}\", '%b %d, %Y %I:%M %p')\n",
    "        data['Send Date'] = datetime_obj.strftime('%Y-%m-%d')\n",
    "        data['Time'] = datetime_obj.strftime('%H:%M:%S')\n",
    "        data['Send Weekday'] = datetime_obj.strftime('%A')\n",
    "        try:\n",
    "            raised_element = WebDriverWait(driver, 10).until(\n",
    "                EC.visibility_of_element_located((By.XPATH, '//div[text()=\"Raised\"]/preceding-sibling::div'))\n",
    "            )\n",
    "            data['Raised'] = raised_element.text.strip()\n",
    "        except NoSuchElementException:\n",
    "            print(f\"'Raised' data not found for {url}\")\n",
    "            data['Raised'] = 'N/A' \n",
    "\n",
    "        # Click the 'Content' tab\n",
    "        try:\n",
    "            content_tab = WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, '//a[contains(@href, \"/content\") and contains(@class, \"tw-relative\")]'))\n",
    "            )\n",
    "            content_tab.click()\n",
    "        except TimeoutException:\n",
    "            print(\"Content tab not found or not clickable.\")\n",
    "    \n",
    "        # fetch the subject\n",
    "        try:\n",
    "            subject_element = WebDriverWait(driver, 10).until(\n",
    "                EC.visibility_of_element_located((By.XPATH, '//span[@data-testid=\"display-subject\"]'))\n",
    "            )\n",
    "            data['Subject'] = subject_element.text.strip()\n",
    "        except TimeoutException:\n",
    "            data['Subject'] = \"Subject not found or empty\"\n",
    "\n",
    "        # Use CSS selectors \n",
    "        # data['Delivered'] = driver.find_element(By.CSS_SELECTOR, 'a.font-w500:contains(\"Delivered\") + div').text.strip()\n",
    "        # data['Opened'] = driver.find_element(By.CSS_SELECTOR, 'a.font-w500:contains(\"Opened\") + div').text.strip()\n",
    "        # data['Clicked'] = driver.find_element(By.CSS_SELECTOR, 'a.font-w500:contains(\"Clicked\") + div').text.strip()\n",
    "        # data['Unsubscribed'] = driver.find_element(By.CSS_SELECTOR, 'a.font-w500:contains(\"Unsubscribed\") + div').text.strip()\n",
    "        # data['Bounced'] = driver.find_element(By.CSS_SELECTOR, 'a.font-w500:contains(\"Bounced\") + div').text.strip()\n",
    "        # data['Raised'] = driver.find_element(By.XPATH, '//div[text()=\"Raised\"]/preceding-sibling::div').text.strip()\n",
    "\n",
    "        \n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping data from {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# List to hold all scraped data\n",
    "all_data = []\n",
    "\n",
    "# Scrape data from each href\n",
    "for href in hrefs:\n",
    "    data = scrape_data(href)\n",
    "    if data:\n",
    "        all_data.append(data)\n",
    "\n",
    "print(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1d56dce4-8d43-40bc-84e2-0e2c4e805e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data to a DataFrame\n",
    "df = pd.DataFrame(all_data)\n",
    "\n",
    "# Save the data to an Excel file\n",
    "df.to_excel('scraped_data.xlsx', index=False)\n",
    "\n",
    "# Save the data to a CSV file\n",
    "df.to_csv('scraped_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea793db-58ac-4584-a090-ba28670758e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df1 = pd.read_excel(r\"path of scraped data file\")\n",
    "df2 = pd.read_csv(r\"path of my mailchimp file\")\n",
    "\n",
    "print(\"File 1\")\n",
    "print(df1.head())\n",
    "print(\"\\nFile 2\")\n",
    "print(df2.head())\n",
    "\n",
    "# Identify common columns (excluding the key column 'Title')\n",
    "common_columns = [col for col in df1.columns if col in df2.columns and col != 'Title']\n",
    "\n",
    "# Include the 'Title' column in the list of columns to retain\n",
    "columns_to_retain = ['Title'] + common_columns\n",
    "\n",
    "# Subset both DataFrames to only include the columns to retain\n",
    "df1_subset = df1[columns_to_retain]\n",
    "df2_subset = df2[columns_to_retain]\n",
    "\n",
    "# Merge the subset DataFrames on the common column 'Title'\n",
    "merged_df = pd.merge(df1_subset, df2_subset, on='Title', how='outer', suffixes=('_df1', '_df2'))\n",
    "\n",
    "# Combine the values for each common column\n",
    "for col in common_columns:\n",
    "    merged_df[col] = merged_df[f\"{col}_df1\"].combine_first(merged_df[f\"{col}_df2\"])\n",
    "    merged_df.drop(columns=[f\"{col}_df1\", f\"{col}_df2\"], inplace=True)\n",
    "\n",
    "# Reorder columns if necessary\n",
    "columns_order = ['Title'] + common_columns\n",
    "merged_df = merged_df[columns_order]\n",
    "\n",
    "print(\"\\nMerged Dataframe\")\n",
    "print(merged_df.head())\n",
    "\n",
    "# Save the merged DataFrame to a new Excel file\n",
    "merged_file_path = 'merged file path'\n",
    "merged_df.to_excel(merged_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1bd36b-809d-4322-a1be-2fd322bddb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the WebDriver\n",
    "driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
